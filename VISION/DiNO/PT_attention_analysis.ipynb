{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harry/miniforge3/envs/aiml/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import Resize\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "model = AutoModel.from_pretrained('facebook/dinov2-base')\n",
    "\n",
    "ds = load_dataset(\"timm/oxford-iiit-pet\")\n",
    "images = ds[\"train\"][\"image\"]\n",
    "inputs = processor(images=images, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the attention heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize an image for display\n",
    "def normalize_for_display(img):\n",
    "    img = img - img.min()\n",
    "    img = img / img.max()\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing attention for image 1...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing attention for image 2...\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAII0lEQVR4nO3XQWpd5wGG4T+NFGHsBuMOQqcdd166g9JddCUhK8lWmlLotGtIKSE0GJPUCEeO3EHgnUoOfJUFzzM+fPzce8597/no3bt37w4AnHN+9dAHAODDIQoARBQAiCgAEFEAIKIAQEQBgIgCALm474V//eKL2SG+On+ebZ/zu+H2d8PtN8PtteXZ3w63nw63r4fbS8vP5Mlw+5zt8/l8uP1ytvz553+88xpvCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYBc3PfCr86fhsf4bLj97XD7dri9dvXQB/iFnjzS7Zvh9vLcHw+3Xw+3zznn+XD7erj9yXD7bt4UAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCALm4/6Uvdqc4b4bbt8Ptt8Ptp8Ptc855Pd5fWd4ry+9zeR9eD7evhts3w+1ztp/5Y/3Nups3BQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEAu7n/p5e4U5z/D7WX3ng23173+9XD75XDb/5j/rxfD7R+G2+ecczPcvh5uL5/Nu3nCAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCALmYXPpB+Xi4fTPcfjrcPuecN8Pt5dl/GG5fDreXrobbr4fba9fD7eXv4cvh9t28KQAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgDk4v6XfrM7xbkdbt8Mt98Mt9e9vhxu/zjcfvJIt5f34dvh9tVwe/n8nLN9hpa/WZ8Ot+/mTQGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKACQi4c+wM/eDLcvh9ufDLc/Hm6fc87r8f7K8n/M9XB7eR8uLZ/N9c/P98Pt5fN5O9y+mzcFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQC7uf+n17hRTnwy3b4fbN8Ptc875abh9Odx+rPfh0tVwe3mPrz0bbi/vwyfD7bt5UwAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgDk4v6Xvtid4i+/321/+ffd9vIzOZfD7XPOeTveX1n+j7kdbi+/z5vh9vLcnw63zznn6+H2s+H26+H23bwpABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFAHJx/0tf7k7x5T932+dyuL30Zry//D9wPdxenvtquP1YP5NXw+33+Pn5RZbf581w++lw+27eFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgC5uP+lT3eneJ9jvLf/DrefD7e/G26fc873w+1nw+2bR7q9dDvcfj7c/nG4fc4518Ptq+H26+H23bwpABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFAHJx/0t/2J1i6rfD7dfD7e+H2+ecczXeX7l96AP8Qo/1/9fyPlw+m+ec82K4/fVw+2Hv8cd6pwIwIAoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBALiaXvrfrR7r9ZLj9Yri99sNw+3a4/VjdDLcvh9trL4fbHw+3l78rd/OmAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoAJCLhz7Azy6H2zfD7d8Mt78dbp9zzpPh9vK/xnL7dri9/Lw/kMf4vS0/73PO+WS4/dNw+3q4fTdvCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYBcPPQBfrY8xh+G238bbq97/XS4fTXcfjXc/nS4fTPc/my4/c1w+9/D7XPOeT7cvh5uXw637+ZNAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoAJCLhz7Az26H2/8Ybj8dbq+9Gm6/HW4vb9lXw+3luf813H7MXg23l9/n1XD7bt4UAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCAPno3bt37x76EAB8GLwpABBRACCiAEBEAYCIAgARBQAiCgBEFACIKACQ/wFchH/Q44V5+gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Global variable to store raw attention weights\n",
    "captured_attention_weights = None\n",
    "\n",
    "# Hook function to capture attention weights\n",
    "def wrapped_attention_hook(module, input, output):\n",
    "    global captured_attention_weights\n",
    "    Q = module.query(input[0])\n",
    "    K = module.key(input[0])\n",
    "\n",
    "    # Compute attention\n",
    "    batch_size, seq_len, hidden_size = Q.shape\n",
    "    num_heads = model.config.num_attention_heads\n",
    "    head_dim = model.config.hidden_size // num_heads\n",
    "\n",
    "    Q = Q.view(batch_size, seq_len, num_heads, head_dim).permute(0, 2, 1, 3)\n",
    "    K = K.view(batch_size, seq_len, num_heads, head_dim).permute(0, 2, 1, 3)\n",
    "    attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (head_dim ** 0.5)\n",
    "    captured_attention_weights = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "# Normalize images for display\n",
    "def normalize_for_display(img):\n",
    "    img = img - img.min()\n",
    "    img = img / img.max()\n",
    "    return img\n",
    "\n",
    "# Function to visualize attention\n",
    "def visualize_attention(image, selected_layer=-1, selected_head=0, threshold=0.8, apply_threshold=True):\n",
    "    global captured_attention_weights\n",
    "\n",
    "    # Clear previous weights\n",
    "    captured_attention_weights = None\n",
    "\n",
    "    # Get the selected layer's attention\n",
    "    layer_attention = model.encoder.layer[selected_layer].attention.attention\n",
    "\n",
    "    # Register the hook and perform forward pass\n",
    "    hook = layer_attention.register_forward_hook(wrapped_attention_hook)\n",
    "    model(image.unsqueeze(0))  # Forward pass\n",
    "    hook.remove()  # Remove the hook\n",
    "\n",
    "    # Verify attention weights are captured\n",
    "    if captured_attention_weights is None:\n",
    "        print(\"Error: Attention weights not captured!\")\n",
    "        return\n",
    "\n",
    "    # Extract attention for the selected head or aggregate across heads\n",
    "    attention_map = captured_attention_weights[0, selected_head, 1:, 1:]  # Patch tokens only\n",
    "\n",
    "    # Aggregate attention for each patch\n",
    "    patch_attention = attention_map.mean(dim=0)  # Average attention across all other patches\n",
    "\n",
    "    # Reshape into a 14x14 grid\n",
    "    patch_attention_2d = patch_attention.view(16, 16)\n",
    "\n",
    "    # Normalize the attention map\n",
    "    patch_attention_2d -= patch_attention_2d.min()\n",
    "    patch_attention_2d /= patch_attention_2d.max()\n",
    "\n",
    "    # Resize to match image dimensions\n",
    "    # resize = Resize((image.shape[-2], image.shape[-1]))\n",
    "    # attention_map_resized = resize(patch_attention_2d.unsqueeze(0).unsqueeze(0))\n",
    "\n",
    "    # Overlay the attention map on the image\n",
    "    # plt.imshow(normalize_for_display(image.permute(1, 2, 0)))  # Original image\n",
    "    # plt.imshow(attention_map_resized.squeeze().detach().cpu().numpy(), cmap=\"jet\", alpha=0.5)  # Overlay heatmap\n",
    "    plt.imshow(patch_attention_2d.squeeze().detach().cpu().numpy(), cmap=\"jet\", alpha=0.5)  # Overlay heatmap\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Analyze attention for different images\n",
    "images = [inputs[\"pixel_values\"][122, :, :, :], inputs[\"pixel_values\"][123, :, :, :]]\n",
    "\n",
    "for idx, img in enumerate(images):\n",
    "    print(f\"Visualizing attention for image {idx + 1}...\")\n",
    "    visualize_attention(img, selected_layer=-1, selected_head=8, threshold=0.8, apply_threshold=False)  # Change head/layer as needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
