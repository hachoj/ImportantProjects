{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What\n",
    "This notebook is a quick research project I perfomed to test the efficacy of knowledge distilattion for \"tiny\" models. The model was close to the GPT-2 model in many parameters with some modern techniques added.\n",
    "- thing 1\n",
    "- thing 2\n",
    "- thing 3\n",
    "- etc.\n",
    "\n",
    "To keep variables to minimum even if the model is not optimal in all aspects, no changed will be done between the two runs except for the way the models are trained.\n",
    "\n",
    "One model will be trained traiditionally using the one-hot encodings for the next token.\n",
    "\n",
    "The other will be trained using a softened output of the llama3.2-1B model.\n",
    "\n",
    "Both models we will evaluated during and after training on zero-shot and few-shot hellaswag, snli, and GLUE benchmarks to evaluate perfomance. Comparing the loss in this scenario is not relevant as the loss calculations are completely different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
