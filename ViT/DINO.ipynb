{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import Callable\n",
    "from ViT import VissionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These augmentations are defined exactly as proposed in the paper\n",
    "def global_augment(images):\n",
    "    global_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224, scale=(0.4, 1.0)),  # Larger crops\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),  # Color jittering\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.GaussianBlur(kernel_size=23, sigma=(0.1, 2.0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    return torch.stack([global_transform(img) for img in images])\n",
    "\n",
    "def multiple_local_augments(images, num_crops=6):\n",
    "    size = 96  # Smaller crops for local\n",
    "    local_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size, scale=(0.05, 0.4)),  # Smaller, more concentrated crops\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(0.4, 0.4, 0.4, 0.1),  # Same level of jittering\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.GaussianBlur(kernel_size=23, sigma=(0.1, 2.0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    # Apply the transformation multiple times to the same image\n",
    "    return torch.stack([local_transform(img) for img in images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "class DINO(nn.Module):\n",
    "    def __init__(self, student_arch: Callable, teacher_arch: Callable, device: torch.device):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            student_arch (nn.Module): ViT Network for student_arch\n",
    "            teacher_arch (nn.Module): ViT Network for teacher_arch\n",
    "            device: torch.device ('cuda' or 'cpu')\n",
    "        \"\"\"\n",
    "        super(DINO, self).__init__()\n",
    "\n",
    "        self.student = student_arch().to(device)\n",
    "        self.teacher = teacher_arch().to(device)\n",
    "        self.teacher.load_state_dict(self.student.state_dict())\n",
    "\n",
    "        # Initialize center as nuffer to avoid backpropagation\n",
    "        self.register_buffer('center', torch.zeros(1, student_arch().output_dim))\n",
    "\n",
    "        for param in self.teacher.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    @staticmethod\n",
    "    def distillication_loss(student_logits, teacher_logits, center, tau_s, tau_t):\n",
    "        \"\"\"\n",
    "        Creating the centered and sharpened loss function to evaluate the student's performance\n",
    "\n",
    "        NOTE:\n",
    "        \"\"\"\n",
    "        # Detatching teacher logits to stop gradients from flowing back into the teacher\n",
    "        teacher_logits = teacher_logits.detach()\n",
    "\n",
    "        # Center and sharpen the teacher's logits\n",
    "        teacher_probs = F.softmax((teacher_logits - center) / tau_t, dim=1)\n",
    "\n",
    "        # Sharpen the student's logits\n",
    "        student_probs = F.log_softmax(student_logits / tau_s, dim=1)\n",
    "\n",
    "        # Calculate cross-entropy loss between the student's and teacher's probs\n",
    "        loss = - (teacher_probs * student_probs).sum(dim=1).mean()\n",
    "        return loss\n",
    "\n",
    "    def teacher_update(self, beta: float):\n",
    "        for teacher_params, student_params in zip(self.teacher.parameters(), self.student.parameters()):\n",
    "            teacher_params.data.mul_(beta).add_(student_params.data, alpha=(1 - beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ViT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m dino \u001b[38;5;241m=\u001b[39m DINO(\u001b[43mViT\u001b[49m(), ViT(), device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ViT' is not defined"
     ]
    }
   ],
   "source": [
    "student = VissionTransformer()\n",
    "teacher = VissionTransformer()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "dino = DINO(student, teacher, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_PT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
